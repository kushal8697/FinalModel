{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "sourceId": 939937,
          "sourceType": "datasetVersion",
          "datasetId": 501529
        },
        {
          "sourceId": 956177,
          "sourceType": "datasetVersion",
          "datasetId": 519884
        },
        {
          "sourceId": 6037307,
          "sourceType": "datasetVersion",
          "datasetId": 3382782
        },
        {
          "sourceId": 8436676,
          "sourceType": "datasetVersion",
          "datasetId": 5025129
        },
        {
          "sourceId": 8436787,
          "sourceType": "datasetVersion",
          "datasetId": 5025211
        },
        {
          "sourceId": 504,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 381,
          "modelId": 34
        }
      ],
      "dockerImageVersionId": 30497,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushal8697/FinalModel/blob/main/Final_of_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = '140k-real-and-fake-faces:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F501529%2F939937%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240909%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240909T055927Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D067dd3e1c366bbc3fbc150ce1f0c83400d4986a60df6e77f6404df047d9ea95c2be9c2ca4e7b2e62cb4bd07c181fe3bce079047b7a4233440750f13c170fc39e406a8f0424b6189c1fc28d2ef42e8c4a86feb43aa2b6d0073833c9df15341f32bed4aa1e3555cfa58ec673943e776cd03cfdbea4b48df00f18cf4ea314aa106f1a119d2fda087dae52c846dfe3e5ec381a0d4f668167bbc037eb4f89eaf9895bd945a7e613110c0997e204cc917e0ea06125b4a6d981d4b20ccca1cbf7bcb7cf55ddd0b122abc5207897c669755d76f026c1dddcdb6be45a6ea0465c08ac16ebadc6c6529aca4c298fc6cbcbc054918ea77a4eba8b0e2a13ef0c0cae1a9f9bcf,faceforensics:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F519884%2F956177%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240909%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240909T055927Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db4517851e300b410c23313518c1e6881ea5595395d45513855651a7da5c49f71f763c5cb2adc375f5617022ec5cf4e2b7765c42cd8686c5925d0e94ca15e4c41d7a1ea2b156b3052bdc58d95b3cb7bbe92632c96f7738e25e68f4c9f421117f14b74b59926ffa9b7171c2147ddc46fcc7cf770d85fa5a2e4fe28d47f9be95fa7d050e9989626e38f9ca14b10ff476c5b0cc443645c1e47806fb732bae445e96aa6fea889a05de2eba03bf47772e5183d9d5d34d9f970428f9b054445293d57fdb785276d224e70f16b40c94abe7932cef938f1e5b91b3a72b71b46722f455dc5d1ba5d8ee02638ded1d8c9c4df5d79f187018cfffd7a909b59dc97780c8f2770,modelos:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3382782%2F6037307%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240909%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240909T055927Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Daf44b165ab42a318a1ed4f42963e3ed2daf85cce4ff92275477f1101579dd09545fc6f55f141c86d881e31e0be0deb9ff1252bb98e9e8983070dd7efd85987d33c8d0b7d7e5933fe6ca70af503fe60d4dfa9b5d4d58abe9f584afa7e1b85e30f15cc8bf278f8f8cd6ba75d02a7050123165d9e35c7e4940895c9365e120700fbcb634a66ff01f255b90f8622813c8bd4fa3720126e61be94affe5748fe9b391978bcde0dc0c28366eccb42b84bf1a4c4a0499bb61c272392a60846d72089d7804014c3948bce0e757ef2a77df6e55ff4ddd893dc61ec976bf28afa187056f0eaf448b4fadf31ba97e16fec3ffb568c3a1a0b8f4d5bb361839864a8db16850745,convnext/tensorflow2/tiny-1k-224/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F381%2F504%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240909%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240909T055928Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dc2e18fe7ebfac168b2886998b78b199e072fe0ea61eaf86d89b811f73c3dd861f535c2f6eb5ced1756bdfac65c4842946ab30230d51ebdead166ace54a7264b99bc045641cc2acfd4bca127ae94dae19485eb3e843cbde25d4c03488a733ee26b76536ddc4c286c950da9935fdb535ea47721c9c72840d79f00dc9715a2e742eee62d8f9fab96164663de65265e09c0344a3f3d72ca9fe6a5cb569d58039ecf829fc1d5e791adf21aff0e3a19fcc581dbf10122c7c46ebe9471c23e2fd6c624b64e72d8e1c3d8ee1dcda5ed2aeab7a8967031db26424787f8cfceb2c5a69d8e01e0f0fc90f6003af734c173e331bed85445b1328c37ca63d3f66cb13c93dc312'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "KxM4sCKCXbzA",
        "outputId": "1321d21b-5e32-4feb-94fd-d6a1bf579045",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 140k-real-and-fake-faces, 4024555718 bytes compressed\n",
            "[==================================================] 4024555718 bytes downloaded\n",
            "Downloaded and uncompressed: 140k-real-and-fake-faces\n",
            "Downloading faceforensics, 724841395 bytes compressed\n",
            "[==================================================] 724841395 bytes downloaded\n",
            "Downloaded and uncompressed: faceforensics\n",
            "Downloading modelos, 532033406 bytes compressed\n",
            "[==================================================] 532033406 bytes downloaded\n",
            "Downloaded and uncompressed: modelos\n",
            "Downloading convnext/tensorflow2/tiny-1k-224/1, 106802255 bytes compressed\n",
            "[==================================================] 106802255 bytes downloaded\n",
            "Downloaded and uncompressed: convnext/tensorflow2/tiny-1k-224/1\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Algorithm for Detection of DeepFake Face Images through Machine Learning"
      ],
      "metadata": {
        "id": "-tQxEUUnXbzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Introduction**"
      ],
      "metadata": {
        "id": "fC_8wCxyXbzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tqdm"
      ],
      "metadata": {
        "trusted": true,
        "id": "V237s_0kXbzP",
        "outputId": "27531d0e-0275-4396-ee44-8f6ef3aa241b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm"
      ],
      "metadata": {
        "id": "qpG0gy6XlU9v",
        "outputId": "89d44bb5-a208-41c4-efec-710a6659feb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.6)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "Successfully installed timm-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install seaborn"
      ],
      "metadata": {
        "trusted": true,
        "id": "sQVXiO83XbzQ",
        "outputId": "0f5fa2d6-097e-4e33-a166-5de12d015ab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.1.4)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "trusted": true,
        "id": "KUy-Gg_yXbzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import seaborn as sb\n",
        "import torch\n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
        "from torchvision.utils import make_grid\n",
        "from torch.autograd import Variable\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\n",
        "import shutil  ##offers high-level operation on a file like a copy, create, and remote operation on the file\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "code_dir = \"/kaggle/working/code\"\n",
        "model_dir = \"/kaggle/working/model\"\n",
        "output_dir = \"/kaggle/working/output\"\n",
        "\n",
        "if not os.path.exists(code_dir):\n",
        "    os.mkdir(code_dir)\n",
        "\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "shutil.copyfile(src=\"/kaggle/input/modelos/convnext.py\",\n",
        "                dst=\"/kaggle/working/code/convnext.py\")\n",
        "shutil.copyfile(src=\"/kaggle/input/modelos/convnext_tiny_1k_224_ema.pth\",\n",
        "                dst=\"/kaggle/working/model/convnext_tiny_1k_224_ema.pth\")\n",
        "shutil.copyfile(src=\"/kaggle/input/modelos/vit_b_16-c867db91.pth\",\n",
        "                dst=\"/kaggle/working/model/vit_b_16-c867db91.pth\")\n",
        "\n",
        "os.chdir(\"/kaggle/working/code\")\n",
        "\n",
        "\n",
        "from convnext import ConvNeXt\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "x-vYFWVWXbzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Database**"
      ],
      "metadata": {
        "id": "44ah-_1-XbzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1. Network Architectures**"
      ],
      "metadata": {
        "id": "3t5dkNooXbzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiating the ConvNeXt model\n",
        "def ConvNeXt_model():\n",
        "    model_conv=ConvNeXt()\n",
        "    state_dict = torch.load('/kaggle/working/model/convnext_tiny_1k_224_ema.pth')\n",
        "    model_conv.load_state_dict(state_dict[\"model\"])\n",
        "\n",
        "    return model_conv\n",
        "\n",
        "def ViT_model():\n",
        "    model_vit=vit_b_16(pretrained=True)\n",
        "    return model_vit\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "OTBWjSdUXbzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "LXAFAGkoXbzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "trusted": true,
        "id": "KGgJipcxXbzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
      ],
      "metadata": {
        "trusted": true,
        "id": "KKv8g3orXbza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc, teste_acc, train_loss, teste_loss = [], [], [], []\n",
        "train_precision, teste_precision, train_recall, teste_recall = [], [], [], []\n",
        "train_f1, teste_f1 = [], []\n",
        "df = pd.DataFrame(columns=['Modelo','Experimento','Epoch', 'Train ACC', 'Train Loss', 'Train F1', 'Test ACC', 'Test Loss', 'Test F1'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "ondqYP07Xbzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Methodology**"
      ],
      "metadata": {
        "id": "foRF9XExXbzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1.  Data Augmentation**"
      ],
      "metadata": {
        "id": "5thSqUDUXbzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1.1. Image Processing**"
      ],
      "metadata": {
        "id": "GRJetVEFXbzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def full_data_transform(model_type, data_fraction, batch_size):\n",
        "    # Data transformations\n",
        "    if model_type== 'convnext':\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(256),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    elif model_type == 'vit':\n",
        "        transform = ViT_B_16_Weights.IMAGENET1K_V1.transforms()\n",
        "\n",
        "\n",
        "    local_arquivos='/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'\n",
        "    full_train_dataset = ImageFolder(local_arquivos + \"/train\", transform=transform)\n",
        "    full_test_dataset = ImageFolder(local_arquivos + \"/test\", transform=transform)\n",
        "\n",
        "    # Determine the number of objects to be selected\n",
        "    num_train_data = int(len(full_train_dataset) * data_fraction)\n",
        "    num_test_data = int(len(full_test_dataset) * data_fraction)\n",
        "\n",
        "\n",
        "\n",
        "    # Randomly select objects for the datasets\n",
        "    train_indices = random.sample(range(len(full_train_dataset)), num_train_data)\n",
        "    test_indices = random.sample(range(len(full_test_dataset)), num_test_data)\n",
        "\n",
        "    # Create datasets with randomly selected objects\n",
        "    train_dataset = torch.utils.data.Subset(full_train_dataset, train_indices)\n",
        "    test_dataset = torch.utils.data.Subset(full_test_dataset, test_indices)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=int(batch_size/2), shuffle=False)\n",
        "\n",
        "\n",
        "    return train_dataloader, test_dataloader\n",
        "\n",
        "def ft_data_transform(model_type, data_fraction, batch_size):\n",
        "    # Data transformations\n",
        "    if model_type== 'convnext':\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(256),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "\n",
        "    elif model_type == 'vit':\n",
        "        transform = ViT_B_16_Weights.IMAGENET1K_V1.transforms()\n",
        "\n",
        "    local_arquivos='/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'\n",
        "    full_test_dataset = ImageFolder(local_arquivos + \"/test\", transform=transform)\n",
        "\n",
        "    # Determine the number of objects to be selected\n",
        "    num_test_data = int(len(full_test_dataset) * data_fraction)\n",
        "\n",
        "\n",
        "    # Randomly select objects for the datasets\n",
        "    test_indices = random.sample(range(len(full_test_dataset)), num_test_data)\n",
        "\n",
        "    # Create datasets with randomly selected objects\n",
        "    test_dataset = torch.utils.data.Subset(full_test_dataset, test_indices)\n",
        "\n",
        "    # Create dataloaders\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=int(batch_size/2), shuffle=False)\n",
        "\n",
        "    return test_dataloader"
      ],
      "metadata": {
        "trusted": true,
        "id": "fdNuh8Q4Xbzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1.2. AutoAugment**"
      ],
      "metadata": {
        "id": "26G4QjWdXbze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def AutoAugment_transform(model_type, train_indices, batch_size):\n",
        "    if model_type== 'convnext':\n",
        "        augmentation_transforms = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(256),\n",
        "            transforms.AutoAugment(), ## for automatically enhancing the diversity and quality of the training data\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    elif model_type == 'vit':\n",
        "        augmentation_transforms = transforms.Compose([\n",
        "            transforms.Resize(224, interpolation=Image.BILINEAR),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.AutoAugment(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    local_arquivos='/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'\n",
        "    augmented_train_dataset = ImageFolder(local_arquivos + \"/train\", transform=augmentation_transforms)\n",
        "    augmented_train_dataset = torch.utils.data.Subset(augmented_train_dataset, train_indices)\n",
        "    augmented_train_dataloader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return augmented_train_dataloader\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ti-rzO6PXbzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1.3. RandAugment**"
      ],
      "metadata": {
        "id": "3KFoVGvkXbzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RandAugment_transform(model_type, train_indices, batch_size):\n",
        "    if model_type== 'convnext':\n",
        "        augmentation_transforms = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(256),\n",
        "            transforms.RandAugment(),#RandAugment simplifies the process by using a reduced search space. Instead of searching for the best policy from scratch\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    elif model_type == 'vit':\n",
        "        augmentation_transforms = transforms.Compose([\n",
        "            transforms.Resize(224, interpolation=Image.BILINEAR),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.RandAugment(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    local_arquivos='/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'\n",
        "    augmented_train_dataset = ImageFolder(local_arquivos + \"/train\", transform=augmentation_transforms)\n",
        "    augmented_train_dataset = torch.utils.data.Subset(augmented_train_dataset, train_indices)\n",
        "    augmented_train_dataloader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return augmented_train_dataloader\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-NHk1KgSXbzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1.4. Auto+Rand Augment**"
      ],
      "metadata": {
        "id": "mloTmhcbXbzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Auto_RandAugment_transform(model_type, train_indices, batch_size):\n",
        "    if model_type== 'convnext':\n",
        "        augmentation_transforms = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(256),\n",
        "            transforms.AutoAugment(),\n",
        "            transforms.RandAugment(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    elif model_type == 'vit':\n",
        "        augmentation_transforms = transforms.Compose([\n",
        "            transforms.Resize(224, interpolation=Image.BILINEAR),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.AutoAugment(),\n",
        "            transforms.RandAugment(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    local_arquivos='/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'\n",
        "    augmented_train_dataset = ImageFolder(local_arquivos + \"/train\", transform=augmentation_transforms)\n",
        "    augmented_train_dataset = torch.utils.data.Subset(augmented_train_dataset, train_indices)\n",
        "    augmented_train_dataloader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return augmented_train_dataloader"
      ],
      "metadata": {
        "trusted": true,
        "id": "dBHQxId1Xbzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2. Training and Testing**"
      ],
      "metadata": {
        "id": "UrjxRasmXbzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Generic training function\n",
        "def train(model, dataloader, criterion, optimizer, scheduler, device, ft, epoch, exp, model_type):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    y_true,y_pred=[], []\n",
        "\n",
        "    loop = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "    for batch_idx, (images, labels) in loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad() #Clears the gradients of all optimized tensors before performing backpropagation.\n",
        "        if ft==False:    #Conditionally executes either fine-tuning or regular training based on the ft flag\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad=False   #Freeze model parameters\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.step()\n",
        "\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad=True\n",
        "            #After the optimization step, this loop iterates over all parameters of the model again and sets requires_grad attribute to True. This unfreezes the model's weights and biases, allowing them to be updated during subsequent iterations of training.\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        predicted = outputs.argmax(dim = 1)\n",
        "\n",
        "        y_true.extend(labels.cpu().tolist())\n",
        "        y_pred.extend(predicted.cpu().tolist())\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        loop.set_description(f\"[Epoch {(epoch+1)}]\")\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    if scheduler:\n",
        "        scheduler.step()\n",
        "\n",
        "    train_loss = running_loss / len(dataloader.dataset)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro',zero_division=0)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.6f} | Train Accuracy: {(accuracy * 100):.2f}% | Train F1-Score: {f1:.6f}\")\n",
        "\n",
        "    model_name= f'model_{model_type}_params_exp_{exp}.pth'\n",
        "    torch.save(model.state_dict(), os.path.join('/kaggle/working/model', model_name))\n",
        "\n",
        "    return train_loss, accuracy, f1"
      ],
      "metadata": {
        "trusted": true,
        "id": "ycy8VvCHXbzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic testing function\n",
        "def test(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    y_pred, y_true= [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            predicted = outputs.argmax(dim = 1)\n",
        "\n",
        "            y_true.extend(labels.cpu().tolist())\n",
        "            y_pred.extend(predicted.cpu().tolist())\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_loss = running_loss / len(list(dataloader.dataset))\n",
        "    test_accuracy = accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, test_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro',zero_division=0)\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.6f} | Test Accuracy: {(test_accuracy * 100):.2f}% | Test F1-Score: {test_f1:.6f}\")\n",
        "    return test_loss, test_accuracy, test_f1\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "hyAEnbbEXbzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generic function for training and testing\n",
        "def train_model(model_type, exp,model, train_dataloader, test_dataloader, criterion, optimizer, scheduler, device, num_epochs, ft, num):\n",
        "    model=model.to(device)\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('----------------------------------------------------------------------------')\n",
        "        train_loss, train_accuracy, train_f1 = train(model, train_dataloader, criterion, optimizer, scheduler, device, ft, epoch, exp, model_type)\n",
        "        test_loss, test_accuracy,test_f1 = test(model, test_dataloader, criterion, device)\n",
        "        val=str(num)+str(epoch+1)\n",
        "        df.loc[val]=[model_type, exp, epoch+1, train_accuracy, train_loss, train_f1, test_accuracy, test_loss, test_f1]\n",
        "        df.to_csv('metricas.csv', index = False)\n",
        "        print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "    return train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "EQT8R0l4Xbzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3. Experimental Protocol**"
      ],
      "metadata": {
        "id": "ETcL11kZXbzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying experimental protocol to scenarios resulting from permutations of Data Augmentations (AutoAugment and RandAugment) and Fine-Tuning:\n",
        "1. Without Fine-Tuning\n",
        "2. With Fine-Tuning\n",
        "3. Without Fine-Tuning and with AutoAugment\n",
        "4. With Fine-Tuning and with AutoAugment\n",
        "5. Without Fine-Tuning and with RandAugment\n",
        "6. With Fine-Tuning and with RandAugment\n",
        "7. Without Fine-Tuning and with both AutoAugment and RandAugment\n",
        "8. With Fine-Tuning and with both AutoAugment and RandAugment"
      ],
      "metadata": {
        "id": "EZ-Yn9AmXbzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_scenario(model_type, scenario, data_fraction, num_epochs=10, batch_size=32, learning_rate=0.001):\n",
        "    num_classes = 2\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    num_train_data = int((100000) * data_fraction)\n",
        "    num_test_data = int((20000) * data_fraction)\n",
        "\n",
        "    train_indices = random.sample(range(65000), num_train_data)\n",
        "    test_indices = random.sample(range(20000), num_test_data)\n",
        "\n",
        "    print(f'Number of training objects: {num_train_data}')\n",
        "    print(f'Number of test objects: {num_test_data}')\n",
        "    print(\"============================================================================\")\n",
        "\n",
        "    if scenario == 1:\n",
        "\n",
        "        train_dataloader, test_dataloader= full_data_transform(model_type, data_fraction, batch_size)\n",
        "\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            model = ConvNeXt_model()\n",
        "            model.head = nn.Linear(model.head.in_features, num_classes)  # Substituir num_classes pelo número correto de classes\n",
        "            num=1\n",
        "\n",
        "        elif model_type == 'vit':\n",
        "            model = ViT_model()\n",
        "            model.heads=nn.Linear(768,2)\n",
        "            num=9\n",
        "        else:\n",
        "            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
        "\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            print(\"Experimento 1 - ConvNeXt (Sem Fine-Tuning)\")\n",
        "        elif model_type == 'vit':\n",
        "            print(\"Experimento 1 - ViT (Sem Fine-Tuning)\")\n",
        "\n",
        "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,1, model, train_dataloader, test_dataloader,\n",
        "                                                                                    criterion, optimizer, scheduler, device, 1, False, num)\n",
        "    elif scenario == 2:\n",
        "\n",
        "        train_dataloader, test_dataloader= full_data_transform(model_type, data_fraction, batch_size)\n",
        "\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            model = ConvNeXt_model()\n",
        "            model.head = nn.Linear(model.head.in_features, num_classes)  # Substituir num_classes pelo número correto de classes\n",
        "            num=2\n",
        "\n",
        "        elif model_type == 'vit':\n",
        "            model = ViT_model()\n",
        "            model.heads=nn.Linear(768,2)\n",
        "            num=10\n",
        "        else:\n",
        "            raise ValueError(\"O parâmetro 'model_type' deve ser 'convnext' ou 'vit'.\")\n",
        "\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "             print(\"Experiment 2 - ConvNeXt (With Fine-Tuning)\")\n",
        "        elif model_type == 'vit':\n",
        "            print(\"Experiment 2 - ViT (With Fine-Tuning)\")\n",
        "\n",
        "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,2, model, train_dataloader, test_dataloader,\n",
        "                                                                                    criterion, optimizer, scheduler, device, num_epochs, True, num)\n",
        "    elif scenario == 3:\n",
        "\n",
        "        augmented_train_dataloader = AutoAugment_transform(model_type, train_indices, batch_size)\n",
        "        test_dataloader=ft_data_transform(model_type, data_fraction, batch_size)\n",
        "\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            model = ConvNeXt_model()\n",
        "            model.head = nn.Linear(model.head.in_features, num_classes)  # Substituir num_classes pelo número correto de classes\n",
        "            num=3\n",
        "\n",
        "        elif model_type == 'vit':\n",
        "            model = ViT_model()\n",
        "            model.heads=nn.Linear(768,2)\n",
        "            num=11\n",
        "        else:\n",
        "            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
        "\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            print(\"Experiment 3 - ConvNeXt (No Fine-Tuning with AutoAugment\")\n",
        "        elif model_type == 'vit':\n",
        "            print(\"Experiment 3 - ViT (No Fine-Tuning with AutoAugment)\")\n",
        "\n",
        "\n",
        "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,3, model, augmented_train_dataloader, test_dataloader,\n",
        "                                                                                   criterion, optimizer, scheduler, device, 1, False, num)\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            model = ConvNeXt_model()\n",
        "            model.head = nn.Linear(model.head.in_features, num_classes)  # Replace num_classes with the correct number of classes\n",
        "            num=4\n",
        "\n",
        "        elif model_type == 'vit':\n",
        "            model = ViT_model()\n",
        "            model.heads=nn.Linear(768,2)\n",
        "            num=12\n",
        "        else:\n",
        "            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
        "\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            print(\"Experiment 4 - ConvNeXt (With Fine-Tuning and with AutoAugment)\")\n",
        "        elif model_type == 'vit':\n",
        "            print(\"Experiment 4 - ViT (With Fine-Tuning and with AutoAugment)\")\n",
        "\n",
        "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,4, model, augmented_train_dataloader, test_dataloader,\n",
        "                                                                                   criterion, optimizer, scheduler, device, num_epochs, True, num)\n",
        "\n",
        "    elif scenario == 4:\n",
        "        augmented_train_dataloader = RandAugment_transform(model_type, train_indices, batch_size)\n",
        "        test_dataloader=ft_data_transform(model_type, data_fraction, batch_size)\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            model = ConvNeXt_model()\n",
        "            model.head = nn.Linear(model.head.in_features, num_classes)  # Substituir num_classes pelo número correto de classes\n",
        "            num=5\n",
        "\n",
        "        elif model_type == 'vit':\n",
        "            model = ViT_model()\n",
        "            model.heads=nn.Linear(768,2)\n",
        "            num=13\n",
        "        else:\n",
        "            raise ValueError(\"O parâmetro 'model_type' deve ser 'convnext' ou 'vit'.\")\n",
        "\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            print(\"Experiment 5 - ConvNeXt (No Fine-Tuning and with RandAugment)\")\n",
        "        elif model_type == 'vit':\n",
        "            print(\"Experiment 5 - ViT (No Fine-Tuning and with RandAugment)\")\n",
        "\n",
        "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,5, model, augmented_train_dataloader, test_dataloader,\n",
        "                                                                                   criterion, optimizer, scheduler, device, 1, False, num)\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            model = ConvNeXt_model()\n",
        "            model.head = nn.Linear(model.head.in_features, num_classes)  #Replace num_classes with the correct number of classes\n",
        "            num=6\n",
        "\n",
        "        elif model_type == 'vit':\n",
        "            model = ViT_model()\n",
        "            model.heads=nn.Linear(768,2)\n",
        "            num=14\n",
        "        else:\n",
        "            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
        "\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            print(\"Experiment 6 - ConvNeXt (With Fine-Tuning and with RandAugment)\")\n",
        "        elif model_type == 'vit':\n",
        "            print(\"Experiment 6 - ViT (With Fine-Tuning and with RandAugment)\")\n",
        "\n",
        "\n",
        "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,6, model, augmented_train_dataloader, test_dataloader,\n",
        "                                                                                   criterion, optimizer, scheduler, device, num_epochs, True, num)\n",
        "\n",
        "    elif scenario == 5:\n",
        "        augmented_train_dataloader = Auto_RandAugment_transform(model_type, train_indices, batch_size)\n",
        "        test_dataloader=ft_data_transform(model_type, data_fraction, batch_size)\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            model = ConvNeXt_model()\n",
        "            model.head = nn.Linear(model.head.in_features, num_classes)  # Substituir num_classes pelo número correto de classes\n",
        "            num=7\n",
        "\n",
        "        elif model_type == 'vit':\n",
        "            model = ViT_model()\n",
        "            model.heads=nn.Linear(768,2)\n",
        "            num=15\n",
        "        else:\n",
        "            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
        "\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            print(\"Experiment 7 - ConvNeXt (No Fine-Tuning and with RandAugment and AutoAugment)\")\n",
        "        elif model_type == 'vit':\n",
        "            print(\"Experiment 7 - ViT (No Fine-Tuning and with RandAugment and AutoAugment)\")\n",
        "\n",
        "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,7, model, augmented_train_dataloader, test_dataloader,\n",
        "                                                                                   criterion, optimizer, scheduler, device, 1, False, num)\n",
        "\n",
        "        # Select the model\n",
        "        if model_type == 'convnext':\n",
        "            model = ConvNeXt_model()\n",
        "            model.head = nn.Linear(model.head.in_features, num_classes)\n",
        "            num=8\n",
        "\n",
        "        elif model_type == 'vit':\n",
        "            model = ViT_model()\n",
        "            model.heads=nn.Linear(768,2)\n",
        "            num=16\n",
        "        else:\n",
        "            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
        "\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "\n",
        "        if model_type == 'convnext':\n",
        "            print(\"Experiment 8 - ConvNeXt (With Fine-Tuning and with RandAugment and AutoAugment)\")\n",
        "        elif model_type == 'vit':\n",
        "            print(\"Experiment 8 - ViT (With Fine-Tuning and with RandAugment and AutoAugment)\")\n",
        "\n",
        "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,8, model, augmented_train_dataloader, test_dataloader,\n",
        "                                                                                   criterion, optimizer, scheduler, device, num_epochs, True, num)\n",
        "    else:\n",
        "        raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "KLv5Nq9kXbzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4. Performance Evaluation**"
      ],
      "metadata": {
        "id": "jrz-kBJyXbzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_experiment_graphs(df, experiment_number):\n",
        "    experiment_df = df[df['Experimento'] == experiment_number]\n",
        "\n",
        "    #  Plot epoch x train F1\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(experiment_df['Epoch'], experiment_df['Train F1'], marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Train F1')\n",
        "    plt.title(f'Experiment {experiment_number} - Train F1')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot epoch x train accuracy\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(experiment_df['Epoch'], experiment_df['Train ACC'], marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Train Accuracy')\n",
        "    plt.title(f'Experiment {experiment_number} - Train Accuracy')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot epoch x train loss\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(experiment_df['Epoch'], experiment_df['Train Loss'], marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Train Loss')\n",
        "    plt.title(f'Experimento {experiment_number} - Trai loss')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot epoch x test F1\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(experiment_df['Epoch'], experiment_df['Test F1'], marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Test F1')\n",
        "    plt.title(f'Experiment {experiment_number} - Test F1')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot epoch x test accuracy\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(experiment_df['Epoch'], experiment_df['Test ACC'], marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Test Accuracy')\n",
        "    plt.title(f'Experiment {experiment_number} - Test Accuracy')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot epoch x test loss\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(experiment_df['Epoch'], experiment_df['Test Loss'], marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Test loss')\n",
        "    plt.title(f'Experimento {experiment_number} - Test loss')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_general_graphs(df):\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
        "\n",
        "    # Plot epoch x train F1\n",
        "    ax1 = axes[0, 0]\n",
        "    for experiment_number in df['Experimento'].unique():\n",
        "        experiment_df = df[df['Experimento'] == experiment_number]\n",
        "        ax1.plot(experiment_df['Epoch'], experiment_df['Train F1'], marker='o', label=f'Experimento {experiment_number}')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Train F1')\n",
        "    ax1.set_title('Overall - Train F1')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot epoch x train loss\n",
        "    ax2 = axes[0, 1]\n",
        "    for experiment_number in df['Experiment'].unique():\n",
        "        experiment_df = df[df['Experiment'] == experiment_number]\n",
        "        ax2.plot(experiment_df['Epoch'], experiment_df['Train Loss'], marker='o', label=f'Experiment {experiment_number}')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Train Loss')\n",
        "    ax2.set_title('Overall - Train Loss')\n",
        "    ax2.legend()\n",
        "\n",
        "    # Plot epoch x test F1\n",
        "    ax3 = axes[1, 0]\n",
        "    for experiment_number in df['Experimento'].unique():\n",
        "        experiment_df = df[df['Experimento'] == experiment_number]\n",
        "        ax3.plot(experiment_df['Epoch'], experiment_df['Test F1'], marker='o', label=f'Experimento {experiment_number}')\n",
        "    ax3.set_xlabel('Epoch')\n",
        "    ax3.set_ylabel('Test F1')\n",
        "    ax3.set_title('Overall - Test F1')\n",
        "    ax3.legend()\n",
        "\n",
        "    # Plot epoch x test loss\n",
        "    ax4 = axes[1, 1]\n",
        "    for experiment_number in df['Experiment'].unique():\n",
        "        experiment_df = df[df['Experiment'] == experiment_number]\n",
        "        ax4.plot(experiment_df['Epoch'], experiment_df['Test Loss'], marker='o', label=f'Experimento {experiment_number}')\n",
        "    ax4.set_xlabel('Epoch')\n",
        "    ax4.set_ylabel('Test Loss')\n",
        "    ax4.set_title('Overall - Test Loss')\n",
        "    ax4.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "H2rlCQghXbzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "run_scenario('convnext',1,0.1, num_epochs=10, batch_size=16, learning_rate=0.0001)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "NO8YE5N0Xbzr",
        "outputId": "e6320f1f-cd52-4d98-ecb0-79c26f8a3644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training objects: 10000\n",
            "Number of test objects: 2000\n",
            "============================================================================\n",
            "Experimento 1 - ConvNeXt (Sem Fine-Tuning)\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 1]: 100%|██████████| 625/625 [01:46<00:00,  5.89it/s, loss=0.71]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.043574 | Train Accuracy: 51.21% | Train F1-Score: 0.512008\n",
            "Test Loss: 0.087375 | Test Accuracy: 50.55% | Test F1-Score: 0.505203\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_df = df.groupby(['Modelo', 'Experimento']).mean()\n",
        "print(df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "u2qfOk0JXbzr",
        "outputId": "c6975897-bf01-424d-f422-3a72a892131b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Modelo  Experimento  Epoch  Train ACC  Train Loss  Train F1  Test ACC  \\\n",
            "11  convnext            1      1     0.5121    0.043574  0.512008    0.5055   \n",
            "\n",
            "    Test Loss   Test F1  \n",
            "11   0.087375  0.505203  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_scenario('convnext',2,0.1, num_epochs=10, batch_size=16, learning_rate=0.0001)"
      ],
      "metadata": {
        "trusted": true,
        "id": "1sgxnlB_Xbzs",
        "outputId": "be7383d1-38b2-4297-efc3-64418173810f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training objects: 10000\n",
            "Number of test objects: 2000\n",
            "============================================================================\n",
            "Experiment 2 - ConvNeXt (With Fine-Tuning)\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 1]: 100%|██████████| 625/625 [03:39<00:00,  2.85it/s, loss=0.000668]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.006969 | Train Accuracy: 95.44% | Train F1-Score: 0.954398\n",
            "Test Loss: 0.001356 | Test Accuracy: 99.65% | Test F1-Score: 0.996490\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 2]: 100%|██████████| 625/625 [03:24<00:00,  3.06it/s, loss=0.00185]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.000660 | Train Accuracy: 99.64% | Train F1-Score: 0.996400\n",
            "Test Loss: 0.004834 | Test Accuracy: 98.65% | Test F1-Score: 0.986476\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 3]: 100%|██████████| 625/625 [03:23<00:00,  3.07it/s, loss=0.00404]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.000177 | Train Accuracy: 99.94% | Train F1-Score: 0.999400\n",
            "Test Loss: 0.000597 | Test Accuracy: 99.85% | Test F1-Score: 0.998495\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 4]: 100%|██████████| 625/625 [03:23<00:00,  3.07it/s, loss=0.000679]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.000036 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
            "Test Loss: 0.000526 | Test Accuracy: 99.85% | Test F1-Score: 0.998496\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 5]: 100%|██████████| 625/625 [03:23<00:00,  3.07it/s, loss=0.000416]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.000021 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
            "Test Loss: 0.000504 | Test Accuracy: 99.85% | Test F1-Score: 0.998496\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 6]: 100%|██████████| 625/625 [03:23<00:00,  3.07it/s, loss=0.000107]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.000019 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
            "Test Loss: 0.000478 | Test Accuracy: 99.85% | Test F1-Score: 0.998496\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 7]: 100%|██████████| 625/625 [03:23<00:00,  3.07it/s, loss=0.000393]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.000018 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
            "Test Loss: 0.000475 | Test Accuracy: 99.85% | Test F1-Score: 0.998496\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 8]: 100%|██████████| 625/625 [03:23<00:00,  3.07it/s, loss=0.000149]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.000018 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
            "Test Loss: 0.000472 | Test Accuracy: 99.85% | Test F1-Score: 0.998496\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 9]: 100%|██████████| 625/625 [03:23<00:00,  3.07it/s, loss=0.000316]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.000018 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
            "Test Loss: 0.000472 | Test Accuracy: 99.85% | Test F1-Score: 0.998496\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 10]: 100%|██████████| 625/625 [03:23<00:00,  3.08it/s, loss=9.49e-5]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.000018 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
            "Test Loss: 0.000472 | Test Accuracy: 99.85% | Test F1-Score: 0.998496\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_df = df.groupby(['Modelo', 'Experimento']).mean()\n",
        "print(mean_df)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "CsZceWHDXbzt",
        "outputId": "9d9d6a02-2605-49a7-83b7-fd6d22c99629",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      Epoch  Train ACC  Train Loss  Train F1  Test ACC  \\\n",
            "Modelo   Experimento                                                     \n",
            "convnext 1              1.0    0.51210    0.043574  0.512008    0.5055   \n",
            "         2              5.5    0.99502    0.000795  0.995020    0.9971   \n",
            "\n",
            "                      Test Loss   Test F1  \n",
            "Modelo   Experimento                       \n",
            "convnext 1             0.087375  0.505203  \n",
            "         2             0.001019  0.997093  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_scenario('convnext',3,0.1, num_epochs=3, batch_size=16, learning_rate=0.0001)"
      ],
      "metadata": {
        "trusted": true,
        "id": "I1-o_zFVXbzu",
        "outputId": "be6f9179-618e-4f94-c361-2536364cfd5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training objects: 10000\n",
            "Number of test objects: 2000\n",
            "============================================================================\n",
            "Experiment 3 - ConvNeXt (No Fine-Tuning with AutoAugment\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 1]: 100%|██████████| 625/625 [01:33<00:00,  6.67it/s, loss=0.659]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.042030 | Train Accuracy: 59.64% | Train F1-Score: 0.486636\n",
            "Test Loss: 0.087630 | Test Accuracy: 50.15% | Test F1-Score: 0.475536\n",
            "\n",
            "\n",
            "Experiment 4 - ConvNeXt (With Fine-Tuning and with AutoAugment)\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 1]: 100%|██████████| 625/625 [03:33<00:00,  2.93it/s, loss=0.00345]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.011989 | Train Accuracy: 92.05% | Train F1-Score: 0.879116\n",
            "Test Loss: 0.010821 | Test Accuracy: 96.85% | Test F1-Score: 0.968494\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 2]: 100%|██████████| 625/625 [03:33<00:00,  2.93it/s, loss=0.0648]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.002986 | Train Accuracy: 98.32% | Train F1-Score: 0.976241\n",
            "Test Loss: 0.001942 | Test Accuracy: 99.65% | Test F1-Score: 0.996499\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 3]: 100%|██████████| 625/625 [03:33<00:00,  2.93it/s, loss=0.00141]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.001164 | Train Accuracy: 99.38% | Train F1-Score: 0.991243\n",
            "Test Loss: 0.001057 | Test Accuracy: 99.85% | Test F1-Score: 0.998500\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_df = df.groupby(['Modelo', 'Experimento']).mean()\n",
        "print(mean_df)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9dAM4FlpXbzu",
        "outputId": "32656157-903f-4854-bfbb-54cd8b91be36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      Epoch  Train ACC  Train Loss  Train F1  Test ACC  \\\n",
            "Modelo   Experimento                                                     \n",
            "convnext 1              1.0   0.512100    0.043574  0.512008  0.505500   \n",
            "         2              5.5   0.995020    0.000795  0.995020  0.997100   \n",
            "         3              1.0   0.596400    0.042030  0.486636  0.501500   \n",
            "         4              2.0   0.965833    0.005379  0.948867  0.987833   \n",
            "\n",
            "                      Test Loss   Test F1  \n",
            "Modelo   Experimento                       \n",
            "convnext 1             0.087375  0.505203  \n",
            "         2             0.001019  0.997093  \n",
            "         3             0.087630  0.475536  \n",
            "         4             0.004607  0.987831  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_scenario('convnext',4,0.1, num_epochs=10, batch_size=16, learning_rate=0.0001)"
      ],
      "metadata": {
        "trusted": true,
        "id": "23PT6HbdXbz9",
        "outputId": "5d02adaf-2c6f-4757-8539-9ccda80d036a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training objects: 10000\n",
            "Number of test objects: 2000\n",
            "============================================================================\n",
            "Experiment 5 - ConvNeXt (No Fine-Tuning and with RandAugment)\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 1]: 100%|██████████| 625/625 [01:34<00:00,  6.64it/s, loss=0.738]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.047590 | Train Accuracy: 29.04% | Train F1-Score: 0.284209\n",
            "Test Loss: 0.088286 | Test Accuracy: 47.15% | Test F1-Score: 0.433656\n",
            "\n",
            "\n",
            "Experiment 6 - ConvNeXt (With Fine-Tuning and with RandAugment)\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Epoch 1]: 100%|██████████| 625/625 [03:35<00:00,  2.90it/s, loss=0.00739]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.010994 | Train Accuracy: 92.50% | Train F1-Score: 0.888414\n",
            "Test Loss: 0.003007 | Test Accuracy: 99.10% | Test F1-Score: 0.991000\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 2]: 100%|██████████| 625/625 [03:36<00:00,  2.89it/s, loss=0.00237]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.002493 | Train Accuracy: 98.62% | Train F1-Score: 0.980718\n",
            "Test Loss: 0.001118 | Test Accuracy: 99.80% | Test F1-Score: 0.998000\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 3]: 100%|██████████| 625/625 [03:36<00:00,  2.89it/s, loss=0.00103]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.000612 | Train Accuracy: 99.67% | Train F1-Score: 0.995402\n",
            "Test Loss: 0.002761 | Test Accuracy: 99.10% | Test F1-Score: 0.990998\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 4]: 100%|██████████| 625/625 [03:36<00:00,  2.88it/s, loss=0.00515]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.000305 | Train Accuracy: 99.88% | Train F1-Score: 0.998329\n",
            "Test Loss: 0.001731 | Test Accuracy: 99.45% | Test F1-Score: 0.994499\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 5]: 100%|██████████| 625/625 [03:36<00:00,  2.89it/s, loss=0.000406]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.000249 | Train Accuracy: 99.90% | Train F1-Score: 0.998607\n",
            "Test Loss: 0.001540 | Test Accuracy: 99.45% | Test F1-Score: 0.994499\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 6]: 100%|██████████| 625/625 [03:36<00:00,  2.88it/s, loss=0.00038]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.000222 | Train Accuracy: 99.92% | Train F1-Score: 0.998885\n",
            "Test Loss: 0.001316 | Test Accuracy: 99.50% | Test F1-Score: 0.994999\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 7]: 100%|██████████| 625/625 [03:37<00:00,  2.88it/s, loss=0.000468]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.000189 | Train Accuracy: 99.95% | Train F1-Score: 0.999304\n",
            "Test Loss: 0.001349 | Test Accuracy: 99.50% | Test F1-Score: 0.994999\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 8]: 100%|██████████| 625/625 [03:36<00:00,  2.88it/s, loss=0.00118]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.000187 | Train Accuracy: 99.94% | Train F1-Score: 0.999164\n",
            "Test Loss: 0.001345 | Test Accuracy: 99.50% | Test F1-Score: 0.994999\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 9]: 100%|██████████| 625/625 [03:36<00:00,  2.89it/s, loss=0.000319]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.000266 | Train Accuracy: 99.87% | Train F1-Score: 0.998188\n",
            "Test Loss: 0.001340 | Test Accuracy: 99.50% | Test F1-Score: 0.994999\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 10]: 100%|██████████| 625/625 [03:36<00:00,  2.89it/s, loss=0.00375]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.000262 | Train Accuracy: 99.87% | Train F1-Score: 0.998189\n",
            "Test Loss: 0.001343 | Test Accuracy: 99.50% | Test F1-Score: 0.994999\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_df = df.groupby(['Modelo', 'Experimento']).mean()\n",
        "print(mean_df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ozL5tp91Xbz-",
        "outputId": "ef21eb79-fedc-4538-ff2b-33f57d401514",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      Epoch  Train ACC  Train Loss  Train F1  Test ACC  \\\n",
            "Modelo   Experimento                                                     \n",
            "convnext 1              1.0   0.512100    0.043574  0.512008  0.505500   \n",
            "         2              5.5   0.995020    0.000795  0.995020  0.997100   \n",
            "         3              1.0   0.596400    0.042030  0.486636  0.501500   \n",
            "         4              2.0   0.965833    0.005379  0.948867  0.987833   \n",
            "         5              1.0   0.290400    0.047590  0.284209  0.471500   \n",
            "         6              5.5   0.990120    0.001578  0.985520  0.994400   \n",
            "\n",
            "                      Test Loss   Test F1  \n",
            "Modelo   Experimento                       \n",
            "convnext 1             0.087375  0.505203  \n",
            "         2             0.001019  0.997093  \n",
            "         3             0.087630  0.475536  \n",
            "         4             0.004607  0.987831  \n",
            "         5             0.088286  0.433656  \n",
            "         6             0.001685  0.994399  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_scenario('convnext',5,0.1, num_epochs=10, batch_size=16, learning_rate=0.0001)"
      ],
      "metadata": {
        "trusted": true,
        "id": "QIYvI3DRXbz-",
        "outputId": "9aa649ea-d794-4548-8d53-068b30b5a162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training objects: 10000\n",
            "Number of test objects: 2000\n",
            "============================================================================\n",
            "Experiment 7 - ConvNeXt (No Fine-Tuning and with RandAugment and AutoAugment)\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 1]: 100%|██████████| 625/625 [01:43<00:00,  6.03it/s, loss=0.726]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.045498 | Train Accuracy: 33.74% | Train F1-Score: 0.336746\n",
            "Test Loss: 0.088437 | Test Accuracy: 46.55% | Test F1-Score: 0.422965\n",
            "\n",
            "\n",
            "Experiment 8 - ConvNeXt (With Fine-Tuning and with RandAugment and AutoAugment)\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 1]: 100%|██████████| 625/625 [03:45<00:00,  2.77it/s, loss=0.311]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.014590 | Train Accuracy: 90.19% | Train F1-Score: 0.850568\n",
            "Test Loss: 0.007585 | Test Accuracy: 98.25% | Test F1-Score: 0.982499\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 2]: 100%|██████████| 625/625 [03:45<00:00,  2.77it/s, loss=0.0191]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.004525 | Train Accuracy: 97.32% | Train F1-Score: 0.962442\n",
            "Test Loss: 0.004508 | Test Accuracy: 98.85% | Test F1-Score: 0.988500\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 3]: 100%|██████████| 625/625 [03:45<00:00,  2.77it/s, loss=0.137]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.001770 | Train Accuracy: 99.02% | Train F1-Score: 0.986340\n",
            "Test Loss: 0.001813 | Test Accuracy: 99.40% | Test F1-Score: 0.994000\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 4]: 100%|██████████| 625/625 [03:45<00:00,  2.77it/s, loss=0.0249]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.001317 | Train Accuracy: 99.30% | Train F1-Score: 0.990245\n",
            "Test Loss: 0.001298 | Test Accuracy: 99.40% | Test F1-Score: 0.994000\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 5]: 100%|██████████| 625/625 [03:45<00:00,  2.77it/s, loss=0.00124]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.001145 | Train Accuracy: 99.38% | Train F1-Score: 0.991376\n",
            "Test Loss: 0.001486 | Test Accuracy: 99.45% | Test F1-Score: 0.994500\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 6]: 100%|██████████| 625/625 [03:45<00:00,  2.77it/s, loss=0.00127]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.001069 | Train Accuracy: 99.36% | Train F1-Score: 0.991095\n",
            "Test Loss: 0.001402 | Test Accuracy: 99.45% | Test F1-Score: 0.994500\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 7]: 100%|██████████| 625/625 [03:45<00:00,  2.78it/s, loss=0.0394]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.001101 | Train Accuracy: 99.38% | Train F1-Score: 0.991365\n",
            "Test Loss: 0.001331 | Test Accuracy: 99.45% | Test F1-Score: 0.994500\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 8]: 100%|██████████| 625/625 [03:45<00:00,  2.77it/s, loss=0.0166]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.001060 | Train Accuracy: 99.38% | Train F1-Score: 0.991360\n",
            "Test Loss: 0.001265 | Test Accuracy: 99.45% | Test F1-Score: 0.994500\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 9]: 100%|██████████| 625/625 [03:45<00:00,  2.77it/s, loss=0.0725]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.001078 | Train Accuracy: 99.49% | Train F1-Score: 0.992888\n",
            "Test Loss: 0.001260 | Test Accuracy: 99.45% | Test F1-Score: 0.994500\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 10]: 100%|██████████| 625/625 [03:45<00:00,  2.77it/s, loss=0.00536]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.000853 | Train Accuracy: 99.56% | Train F1-Score: 0.993872\n",
            "Test Loss: 0.001259 | Test Accuracy: 99.45% | Test F1-Score: 0.994500\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_df = df.groupby(['Modelo', 'Experimento']).mean()\n",
        "print(mean_df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "gvhe-2jOXbz_",
        "outputId": "c8d63c63-59ec-494c-fbff-32e737fdc70b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      Epoch  Train ACC  Train Loss  Train F1  Test ACC  \\\n",
            "Modelo   Experimento                                                     \n",
            "convnext 1              1.0   0.512100    0.043574  0.512008  0.505500   \n",
            "         2              5.5   0.995020    0.000795  0.995020  0.997100   \n",
            "         3              1.0   0.596400    0.042030  0.486636  0.501500   \n",
            "         4              2.0   0.965833    0.005379  0.948867  0.987833   \n",
            "         5              1.0   0.290400    0.047590  0.284209  0.471500   \n",
            "         6              5.5   0.990120    0.001578  0.985520  0.994400   \n",
            "         7              1.0   0.337400    0.045498  0.336746  0.465500   \n",
            "         8              5.5   0.982380    0.002851  0.974155  0.992600   \n",
            "\n",
            "                      Test Loss   Test F1  \n",
            "Modelo   Experimento                       \n",
            "convnext 1             0.087375  0.505203  \n",
            "         2             0.001019  0.997093  \n",
            "         3             0.087630  0.475536  \n",
            "         4             0.004607  0.987831  \n",
            "         5             0.088286  0.433656  \n",
            "         6             0.001685  0.994399  \n",
            "         7             0.088437  0.422965  \n",
            "         8             0.002321  0.992600  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('metricas.csv', index = False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "uGnUsVWlXb0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import sequential"
      ],
      "metadata": {
        "id": "WJbHVy6kKf7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(my_model.h5)"
      ],
      "metadata": {
        "id": "x7EMmjBJXb0C",
        "outputId": "2a0e8cf5-1c40-4518-f7b5-73bad6a4b988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-36ca5c179497>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CeJjCG5rKYbL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}